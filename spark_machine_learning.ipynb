{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R77QjAjTB4-2",
        "outputId": "21140dfd-1184-4051-f8dc-47c9153a5c1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=ce0815be1abed97aa9a00ce0a6b9b1eda962acba63d2b272551c62011ff9960a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        " # Connect Google Drive Untuk Ambil Data\n",
        " from google.colab import drive\n",
        " drive.mount('/content/drive')\n",
        "\n",
        " # Install PySpark\n",
        " !pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Library\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Movie Lens\").getOrCreate()\n",
        "\n",
        " # Parse String Menjadi Objek Rating\n",
        "def parseRating(str):\n",
        "    fields = str.split(\",\")\n",
        "    assert len(fields) == 4\n",
        "    return (int(fields[0]), int(fields[1]), float(fields[2]), int(fields[3]))\n",
        "\n",
        "\n",
        " # Baca File\n",
        "raw = spark.read.text(\"/content/drive/MyDrive/S6/big data/ml-latest-small/ratings.dat\").rdd.map(lambda x: x[0])\n",
        "header = raw.first()\n",
        "data = raw.filter(lambda x: x != header)\n",
        "ratings = data.map(parseRating).toDF([\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "\n",
        "\n",
        " # Data Training 80% dan Test 20%\n",
        "training, test = ratings.randomSplit([0.8, 0.2])\n",
        "\n",
        " # Membuat Model\n",
        "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\n",
        "model = als.fit(training)\n",
        "model.save(\"mymodel\")\n",
        "\n",
        " # Prediksi Data\n",
        "predictions = model.transform(test)\n",
        "mse = predictions.withColumn(\"diff\", col(\"rating\") - col(\"prediction\")).select((col(\"diff\") ** 2).alias(\"squared_diff\")).filter(~col(\"squared_diff\").isNull()).agg({\"squared_diff\": \"sum\"}).collect()[0][0]\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "predictions.show(10)\n",
        "\n",
        " # Menyimpan Hasil Prediksi\n",
        "predictions.write.format(\"csv\").save(\"ml-predictions.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLiYGcqLCJof",
        "outputId": "15bfac04-6674-462a-8c47-526f6dfc19e8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: nan\n",
            "+------+-------+------+---------+----------+\n",
            "|userId|movieId|rating|timestamp|prediction|\n",
            "+------+-------+------+---------+----------+\n",
            "|     1|     47|   5.0|964983815|   4.54771|\n",
            "|     1|    101|   5.0|964980868|  3.902245|\n",
            "|     1|    216|   5.0|964981208|  3.605606|\n",
            "|     1|    356|   4.0|964980962| 4.8947506|\n",
            "|     1|    423|   3.0|964982363|  3.355478|\n",
            "|     1|    553|   5.0|964984153| 4.1617055|\n",
            "|     1|    593|   4.0|964983793| 4.9087496|\n",
            "|     1|    596|   5.0|964982838| 4.0692987|\n",
            "|     1|    804|   4.0|964980499| 2.7491038|\n",
            "|     1|    943|   4.0|964983614| 2.4962883|\n",
            "+------+-------+------+---------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Library\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Movie Lens\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Parse String Menjadi Objek Rating\n",
        "def parseRating(str):\n",
        "    fields = str.split(\",\")\n",
        "    assert len(fields) == 4\n",
        "    return (int(fields[0]), int(fields[1]), float(fields[2]), int(fields[3]))\n",
        "\n",
        "# Baca File\n",
        "raw = spark.read.text(\"/content/drive/MyDrive/S6/big data/ml-latest-small/ratings.csv\").rdd.map(lambda x: x[0])\n",
        "header = raw.first()\n",
        "data = raw.filter(lambda x: x != header)\n",
        "ratings = data.map(parseRating).toDF([\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "class Rating:\n",
        "    def __init__(self, userId, movieId, rating):\n",
        "        self.userId = userId\n",
        "        self.movieId = movieId\n",
        "        self.rating = rating\n",
        "\n",
        "ratings_df = ratings.select([\"userId\", \"movieId\", \"rating\"])\n",
        "\n",
        "# Build the recommendation model using ALS\n",
        "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
        "model = als.fit(ratings_df)\n",
        "\n",
        "# Generate product recommendations for user ID 1\n",
        "products = model.recommendForUserSubset(spark.createDataFrame([(1,)]).toDF(\"userId\"), 10)\n",
        "\n",
        "products.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKXFgttdCOP9",
        "outputId": "d11d2d01-02a3-42fe-ad38-c2cfe0b4afad"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+\n",
            "|userId|     recommendations|\n",
            "+------+--------------------+\n",
            "|     1|[{33649, 5.555023...|\n",
            "+------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Library\n",
        "from pyspark.mllib.stat import Statistics\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Movie Lens\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Baca File\n",
        "raw = spark.read.csv(\"/content/drive/MyDrive/S6/big data/ml-latest-small/ratings.csv\", header=True)\n",
        "ratings = raw.selectExpr(\"cast(userId as int) userId\", \"cast(movieId as int) movieId\", \"cast(rating as float) rating\", \"cast(timestamp as int) timestamp\")\n",
        "\n",
        "# Diambil dari ratings.csv\n",
        "mat = ratings.select(\"rating\").rdd.map(lambda x: [x[0]])\n",
        "\n",
        "# Perhitungan Statistics\n",
        "summary = Statistics.colStats(mat)\n",
        "print(\"Mean:\", summary.mean()[0])\n",
        "print(\"Variance:\", summary.variance()[0])\n",
        "print(\"Number of Non-Zeros:\", summary.numNonzeros()[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPRK1YytCT1b",
        "outputId": "d47afe84-652a-4f3b-c629-bb7469e9dcbe"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: 3.5015569836169593\n",
            "Variance: 1.086867214296345\n",
            "Number of Non-Zeros: 100836.0\n"
          ]
        }
      ]
    }
  ]
}